{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607725b9-7398-415f-a526-a8a22350540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shang\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shang\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shang\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\shang\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\shang\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\shang\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\shang\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shang\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!conda install pip -y\n",
    "!pip install -U sentence-transformers\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd530eb-53e5-4fa1-b707-d74bebd01e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29694932-12b6-4bbd-9faa-6ddcbe534dcc",
   "metadata": {},
   "source": [
    "## Install PDF Support (for PyMuPDF)\n",
    "\n",
    "Required for extracting resume text from uploaded PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60f44f8-0e6d-4eb0-8fac-77f2081fbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "#!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524c243-2841-45e9-83bc-b2ca15484017",
   "metadata": {},
   "source": [
    "## Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216b5a1c-27c5-4925-bcd5-235e1889a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume dataset columns: ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
      "Job dataset columns: ['Document ID', 'Job Title', 'Job Description']\n"
     ]
    }
   ],
   "source": [
    "queries_df = pd.read_csv('../data/Resume.csv')  # Resumes as queries\n",
    "documents_df = pd.read_csv('../data/job_title_des.csv')  # Jobs as documents\n",
    "\n",
    "print(\"Resume dataset columns:\", queries_df.columns.tolist())\n",
    "print(\"Job dataset columns:\", documents_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2d6e5-6870-4ab9-868f-38be49e1db4f",
   "metadata": {},
   "source": [
    "## Filter Resumes for Tech-Related Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7feb6353-eb16-4660-8d86-0183fe4e04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df['Category'] = queries_df['Category'].str.upper().str.strip()\n",
    "target_categories = ['INFORMATION-TECHNOLOGY']  \n",
    "filtered_queries = queries_df[queries_df['Category'].isin(target_categories)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992730f7-8240-4ab0-8e82-c5c1c0ce63ca",
   "metadata": {},
   "source": [
    "## Basic Text Preprocessing\n",
    "\n",
    "- Convert all text to lowercase for consistency.\n",
    "- Strip leading and trailing whitespace.\n",
    "- Apply these cleaning steps to relevant text columns (e.g., job descriptions, resume text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0248e1-b71d-4fe9-969f-0265fab873c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().strip()\n",
    "    return \"\"\n",
    "documents_df['cleaned_description'] = documents_df['Job Description'].apply(clean_text)\n",
    "filtered_queries['cleaned_text'] = filtered_queries['Resume_str'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569410a-8707-49d3-becd-042967b7265b",
   "metadata": {},
   "source": [
    "## PDF Resume Text Extraction and Preprocessing\n",
    "\n",
    "This section defines a helper function to extract text from a PDF resume and clean it using the same preprocessing as the existing resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93aae99-a6e5-414a-8604-e871ddb28529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "def extract_and_clean_pdf_resume(pdf_path):\n",
    "    \"\"\"Extract and clean text from a PDF resume using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return clean_text(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db945bb-c5d2-49b1-b5c9-5edc7dee7dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumes:\n",
      "           ID\n",
      "217  36856210\n",
      "218  21780877\n",
      "219  33241454\n",
      "220  25990239\n",
      "221  16899268\n",
      "                                          cleaned_text\n",
      "217  information technology         summary     ded...\n",
      "218  information technology specialist\\tgs11       ...\n",
      "219  information technology supervisor       summar...\n",
      "220  information technology instructor       summar...\n",
      "221  information technology manager/analyst        ...\n",
      "                   Category\n",
      "217  INFORMATION-TECHNOLOGY\n",
      "218  INFORMATION-TECHNOLOGY\n",
      "219  INFORMATION-TECHNOLOGY\n",
      "220  INFORMATION-TECHNOLOGY\n",
      "221  INFORMATION-TECHNOLOGY\n",
      "\n",
      "Docs:\n",
      "              Job Title                                cleaned_description\n",
      "0     Flutter Developer  we are looking for hire experts flutter develo...\n",
      "1      Django Developer  python/django (developer/lead) - job code(pdj ...\n",
      "2      Machine Learning  data scientist (contractor)\\n\\nbangalore, in\\n...\n",
      "3         iOS Developer  job description:\\n\\nstrong framework outside o...\n",
      "4  Full Stack Developer  job responsibility full stack engineer â€“ react...\n"
     ]
    }
   ],
   "source": [
    "# Print Sample rows \n",
    "print(\"Resumes:\")\n",
    "print(filtered_queries[['ID']].head())\n",
    "print(filtered_queries[['cleaned_text']].head())\n",
    "print(filtered_queries[['Category']].head())\n",
    "\n",
    "print(\"\\nDocs:\")\n",
    "print(documents_df[['Job Title', 'cleaned_description']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683eaab-bb3f-408a-8da5-14c8c0f1d8e2",
   "metadata": {},
   "source": [
    "## Switching to Semantic Retrieval (LLM-based)\n",
    "\n",
    "In this section, we transition to a **semantic retrieval approach** using a pre-trained transformer model (`all-MiniLM-L6-v2`). Instead of relying on exact token overlap, this method captures **semantic meaning** to compute similarity between resumes and job descriptions in embedding space.\n",
    "\n",
    "### Key Difference:\n",
    "- **TF-IDF**: Lexical overlap-based\n",
    "- **LLM Embeddings**: Context-aware, meaning-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8af7b17f-f869-419d-ae3a-687b1a2b5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shang\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "llm_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode job descriptions (documents)\n",
    "job_ids = documents_df['Document ID'].tolist()\n",
    "job_titles = documents_df['Job Title'].tolist()\n",
    "job_texts = documents_df['cleaned_description'].tolist()\n",
    "job_embeddings = llm_model.encode(job_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "# Encode resumes (queries)\n",
    "resume_ids = filtered_queries['ID'].tolist() if 'ID' in filtered_queries.columns else list(range(len(filtered_queries)))\n",
    "resume_texts = filtered_queries['cleaned_text'].tolist()\n",
    "resume_embeddings = llm_model.encode(resume_texts, convert_to_tensor=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf899a-bd3b-4850-98ab-ccd59338a500",
   "metadata": {},
   "source": [
    "## LLM: Match Jobs to Uploaded PDF Resume\n",
    "\n",
    "This function uses semantic similarity (SentenceTransformers) to match a user-uploaded PDF resume to the most relevant job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b386bdb6-2351-48e0-9b1b-a487f52b84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pdf_resume_llm(pdf_path, job_df, llm_model, job_embeddings, top_k=10):\n",
    "    cleaned_resume = extract_and_clean_pdf_resume(pdf_path)\n",
    "\n",
    "    resume_embedding = llm_model.encode(cleaned_resume, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    from sentence_transformers import util\n",
    "    cosine_scores = util.cos_sim(resume_embedding, job_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "    print(\"=== LLM Top Job Matches ===\")\n",
    "    for rank, (score, idx) in enumerate(zip(top_results.values, top_results.indices), start=1):\n",
    "        job_idx = idx.item()  # Convert tensor to int\n",
    "        title = job_df.iloc[job_idx]['Job Title']\n",
    "        print(f\"{rank}. {title} (Score: {score.item():.4f})\")\n",
    "        \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82491a71-0ed9-464f-8858-624914d63e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_llm_results_for_all_resumes(filtered_queries, documents_df, llm_model, job_embeddings, output_path=\"llm_resume_to_jobs.csv\", top_k=10):\n",
    "    from sentence_transformers import util\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "\n",
    "    resume_ids = filtered_queries['ID'].tolist() if 'ID' in filtered_queries.columns else list(range(len(filtered_queries)))\n",
    "    resume_texts = filtered_queries['cleaned_text'].tolist()\n",
    "    \n",
    "    output_rows = []\n",
    "\n",
    "    for i, resume_text in enumerate(resume_texts):\n",
    "        resume_id = resume_ids[i]\n",
    "\n",
    "        # Encode the resume as a query\n",
    "        resume_embedding = llm_model.encode(resume_text, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "        # Compute cosine similarity to all job embeddings\n",
    "        cosine_scores = util.cos_sim(resume_embedding, job_embeddings)[0]\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "        for rank, (score, idx) in enumerate(zip(top_results.values, top_results.indices), start=1):\n",
    "            idx = int(idx)  # convert from tensor to int\n",
    "            job_id = documents_df.iloc[idx]['Document ID']\n",
    "            job_title = documents_df.iloc[idx]['Job Title']\n",
    "\n",
    "            output_rows.append({\n",
    "                'Resume_ID': resume_id,\n",
    "                'Rank': rank,\n",
    "                'Matched_Job_ID': job_id,\n",
    "                'Job_Title': job_title,\n",
    "                'LLM_Score': round(score.item(), 4)\n",
    "            })\n",
    "\n",
    "    llm_output_df = pd.DataFrame(output_rows)\n",
    "    llm_output_df.to_csv(output_path, index=False)\n",
    "    print(f\"LLM-based matches saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "421338bc-6b30-4dae-b1be-66db67a1c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_job_to_resumes_llm(job_index, job_text, job_title, resume_df, llm_model, resume_embeddings, top_k=10):\n",
    "    \"\"\"\n",
    "    Given a job description, find top-matching resumes using LLM embeddings.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import util\n",
    "    import torch\n",
    "\n",
    "    # Encode the job description\n",
    "    job_embedding = llm_model.encode(job_text, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    # Compute cosine similarity between job and all resume embeddings\n",
    "    cosine_scores = util.cos_sim(job_embedding, resume_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "    print(f\"\\n=== LLM Top Resume Matches for Job #{job_index}: {job_title} ===\")\n",
    "    for rank, (score, idx) in enumerate(zip(top_results.values, top_results.indices), start=1):\n",
    "        resume_id = resume_df.iloc[idx.item()][\"ID\"]\n",
    "        print(f\"{rank}. Resume ID: {resume_id} (Score: {score.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "928be15c-10cd-43fc-bfe0-cb10595bda06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM Top Resume Matches for Job #0: Flutter Developer ===\n",
      "1. Resume ID: 39413067 (Score: 0.5122)\n",
      "2. Resume ID: 37242217 (Score: 0.4407)\n",
      "3. Resume ID: 11580408 (Score: 0.3829)\n",
      "4. Resume ID: 17641670 (Score: 0.3729)\n",
      "5. Resume ID: 26480367 (Score: 0.3721)\n",
      "6. Resume ID: 36434348 (Score: 0.3617)\n",
      "7. Resume ID: 15651486 (Score: 0.3609)\n",
      "8. Resume ID: 25207620 (Score: 0.3573)\n",
      "9. Resume ID: 22450718 (Score: 0.3555)\n",
      "10. Resume ID: 37764298 (Score: 0.3544)\n"
     ]
    }
   ],
   "source": [
    "# Test LLM-based matching from job to resumes\n",
    "sample_job = documents_df.iloc[0]\n",
    "job_index = sample_job['Document ID']\n",
    "job_text = sample_job['cleaned_description']\n",
    "job_title = sample_job['Job Title']\n",
    "\n",
    "match_job_to_resumes_llm(\n",
    "    job_index=job_index,\n",
    "    job_text=job_text,\n",
    "    job_title=job_title,\n",
    "    resume_df=filtered_queries,\n",
    "    llm_model=llm_model,\n",
    "    resume_embeddings=resume_embeddings,\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c033738-a0bf-4b0e-a0f0-f6baaedbd042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Top Job Matches for Resume ID: 36856210 ===\n",
      "1. Network Administrator (Score: 0.6007)\n",
      "2. Network Administrator (Score: 0.5993)\n",
      "3. Software Engineer (Score: 0.5954)\n",
      "4. Network Administrator (Score: 0.5944)\n",
      "5. Network Administrator (Score: 0.5876)\n",
      "6. Network Administrator (Score: 0.5853)\n",
      "7. Network Administrator (Score: 0.5826)\n",
      "8. Database Administrator (Score: 0.5802)\n",
      "9. Network Administrator (Score: 0.5719)\n",
      "10. Database Administrator (Score: 0.5686)\n"
     ]
    }
   ],
   "source": [
    "# Test LLM matching from a resume to job descriptions\n",
    "resume_row = filtered_queries.iloc[0]\n",
    "resume_id = resume_row[\"ID\"]\n",
    "resume_text = resume_row[\"cleaned_text\"]\n",
    "\n",
    "# Encode the resume\n",
    "resume_embedding = llm_model.encode(resume_text, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_scores = util.cos_sim(resume_embedding, job_embeddings)[0]\n",
    "top_results = torch.topk(cosine_scores, k=10)\n",
    "\n",
    "print(f\"=== LLM Top Job Matches for Resume ID: {resume_id} ===\")\n",
    "for rank, (score, idx) in enumerate(zip(top_results.values, top_results.indices), start=1):\n",
    "    job_title = documents_df.iloc[idx.item()][\"Job Title\"]\n",
    "    print(f\"{rank}. {job_title} (Score: {score.item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7602d-67b3-4c41-9658-887d67c64b4e",
   "metadata": {},
   "source": [
    "## Save LLM Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d01071b-b87f-4e9f-9c52-2224bf8a6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-based matches saved to 'llm_resume_to_jobs.csv'\n"
     ]
    }
   ],
   "source": [
    "save_llm_results_for_all_resumes(filtered_queries, documents_df, llm_model, job_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
